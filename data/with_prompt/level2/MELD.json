[
    {
        "id": "0",
        "video": "MELD/videos/test_dia240_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Who told you that? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "1",
        "video": "MELD/videos/test_dia7_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: So you can like, bite, and pull people’s hair and stuff? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "2",
        "video": "MELD/videos/test_dia241_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "3",
        "video": "MELD/videos/test_dia124_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Chandler! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "4",
        "video": "MELD/videos/test_dia195_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "5",
        "video": "MELD/videos/test_dia145_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey Chandler, what are you doing here? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "6",
        "video": "MELD/videos/test_dia175_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay.  Oh wait! Umm, did you send those contracts to Milan? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "7",
        "video": "MELD/videos/test_dia173_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "8",
        "video": "MELD/videos/test_dia56_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Wow! I can’t believe that’s our baby. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "9",
        "video": "MELD/videos/test_dia34_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God!! Do it!! Honey, you’ve waited You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "10",
        "video": "MELD/videos/test_dia278_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That’s not true, there are great pictures of us! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "11",
        "video": "MELD/videos/test_dia144_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "12",
        "video": "MELD/videos/test_dia113_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my—Oh good God! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "13",
        "video": "MELD/videos/test_dia39_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God! Monica! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "14",
        "video": "MELD/videos/test_dia181_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You talked to him. Are you crazy? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "15",
        "video": "MELD/videos/test_dia22_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why, exactly? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "16",
        "video": "MELD/videos/test_dia262_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What?! Oh. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "17",
        "video": "MELD/videos/test_dia29_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my god. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "18",
        "video": "MELD/videos/test_dia58_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Right! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "19",
        "video": "MELD/videos/test_dia270_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The uh, the baby that hasn't been born yet? Wouldn't that mean you're... crazy? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "20",
        "video": "MELD/videos/test_dia213_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "21",
        "video": "MELD/videos/test_dia123_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What? So now I’m not allowed to fire him? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "22",
        "video": "MELD/videos/test_dia125_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Like they’re taking it off the air? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "23",
        "video": "MELD/videos/test_dia77_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, where in the hell did you get that?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "24",
        "video": "MELD/videos/test_dia211_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Joey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "25",
        "video": "MELD/videos/test_dia34_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Monica. You’ve, you’ve done it right? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "26",
        "video": "MELD/videos/test_dia175_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: If this is your idea of sexy talk? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "27",
        "video": "MELD/videos/test_dia189_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really? To what? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "28",
        "video": "MELD/videos/test_dia112_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "29",
        "video": "MELD/videos/test_dia107_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Come on you gotta have one! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "30",
        "video": "MELD/videos/test_dia133_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Whoa! What?! Why?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "31",
        "video": "MELD/videos/test_dia169_utt21.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God! I know who the father is You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "32",
        "video": "MELD/videos/test_dia185_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "33",
        "video": "MELD/videos/test_dia94_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "34",
        "video": "MELD/videos/test_dia32_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yes!! Yes! Yes! Yes!! That’s my Dad, that’s Frank! Yeah! I’m sorry I’m getting all flingy. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "35",
        "video": "MELD/videos/test_dia184_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "36",
        "video": "MELD/videos/test_dia145_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Uh, are you crazy? Are you insane? If you live with Phoebe, you two are gonna be y'know, live-living together! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "37",
        "video": "MELD/videos/test_dia9_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Excuse me? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "38",
        "video": "MELD/videos/test_dia119_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh come on, by age seven kids have already seen orgies.  Was it just me?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "39",
        "video": "MELD/videos/test_dia152_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, yeah. You can do it to. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "40",
        "video": "MELD/videos/test_dia72_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: OH….MY….GAWD!!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "41",
        "video": "MELD/videos/test_dia96_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ooh, do I sense a little bit of resentment? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "42",
        "video": "MELD/videos/test_dia16_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: There’s like-there’s like 300 bucks in this one! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "43",
        "video": "MELD/videos/test_dia169_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: There’s a dude? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "44",
        "video": "MELD/videos/test_dia4_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "45",
        "video": "MELD/videos/test_dia88_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Are you intrigued? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "46",
        "video": "MELD/videos/test_dia90_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Good day for married people huh? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "47",
        "video": "MELD/videos/test_dia161_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ahhhh!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "48",
        "video": "MELD/videos/test_dia169_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "49",
        "video": "MELD/videos/test_dia61_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: ...and then Zana, just let one rip!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "50",
        "video": "MELD/videos/test_dia109_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, are you sure? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "51",
        "video": "MELD/videos/test_dia104_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "52",
        "video": "MELD/videos/test_dia158_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Awkward? Why should he feel awkward? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "53",
        "video": "MELD/videos/test_dia257_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You know it? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "54",
        "video": "MELD/videos/test_dia138_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No-no-no-no-no-no-no-no!!! You can't come in here! R-r-r-r-Ross is naked. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "55",
        "video": "MELD/videos/test_dia13_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You're not? See, that's why you're so great! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "56",
        "video": "MELD/videos/test_dia65_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What’s up with Gene? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "57",
        "video": "MELD/videos/test_dia109_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What? What can't you do? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "58",
        "video": "MELD/videos/test_dia268_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, I so can't believe this! My uterus is an inhospitable environment? I was trying so hard to be a good hostess! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "59",
        "video": "MELD/videos/test_dia118_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No-no—Really?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "60",
        "video": "MELD/videos/test_dia138_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "61",
        "video": "MELD/videos/test_dia117_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Who? Me the attacker? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "62",
        "video": "MELD/videos/test_dia205_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I can't believe it! He's looking right at us! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "63",
        "video": "MELD/videos/test_dia252_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "64",
        "video": "MELD/videos/test_dia217_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "65",
        "video": "MELD/videos/test_dia254_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What are you doing? We're going to a hospital. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "66",
        "video": "MELD/videos/test_dia234_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "67",
        "video": "MELD/videos/test_dia126_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay.  What?! What are you doing here?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "68",
        "video": "MELD/videos/test_dia108_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Eh, Monica it, it feels so weird, y'know, Chandler’s your friend...  Oh! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "69",
        "video": "MELD/videos/test_dia110_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: 'Tonight' tonight? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "70",
        "video": "MELD/videos/test_dia83_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why, what happened? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "71",
        "video": "MELD/videos/test_dia272_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, up yours too! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "72",
        "video": "MELD/videos/test_dia231_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I did! But the store wouldn’t take her back! So then I took her to the shelter, and you know what I found out? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "73",
        "video": "MELD/videos/test_dia116_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Isn't! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "74",
        "video": "MELD/videos/test_dia194_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, all right. All right! Just keep walkin'! All right? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "75",
        "video": "MELD/videos/test_dia72_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: And my husband  he’s a lawyer! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "76",
        "video": "MELD/videos/test_dia160_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I mean do we not have as much fun?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "77",
        "video": "MELD/videos/test_dia77_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That cup is mine! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "78",
        "video": "MELD/videos/test_dia105_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Then don't break in!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "79",
        "video": "MELD/videos/test_dia85_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No way! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "80",
        "video": "MELD/videos/test_dia169_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You can’t marry him! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "81",
        "video": "MELD/videos/test_dia112_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The universe said I was going to meet a nice guy and that’s what they gave me? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "82",
        "video": "MELD/videos/test_dia160_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I did not cry my eyes out!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "83",
        "video": "MELD/videos/test_dia168_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The Hardy boys? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "84",
        "video": "MELD/videos/test_dia65_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It’s Clint. It’s Clint! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "85",
        "video": "MELD/videos/test_dia167_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You completely stole my answer! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "86",
        "video": "MELD/videos/test_dia150_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I don’t care!  I don’t care!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "87",
        "video": "MELD/videos/test_dia255_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: WHAT IS IT?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "88",
        "video": "MELD/videos/test_dia205_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No way! Mr.Salty is a sailor, all right, he's got to be, like, thetoughest snack there is. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "89",
        "video": "MELD/videos/test_dia260_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Did you, did you You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "90",
        "video": "MELD/videos/test_dia154_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Y’know she has a face Ross! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "91",
        "video": "MELD/videos/test_dia22_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: If you give up every time you'd have a fight with someone you'd never be with anyone longer than—Ohhh! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "92",
        "video": "MELD/videos/test_dia98_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Power company guys? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "93",
        "video": "MELD/videos/test_dia198_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No-no, she kicked our butts. You could be on the Olympic standing-there team. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "94",
        "video": "MELD/videos/test_dia34_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: There shouldn’t be all this rules and restrictions! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "95",
        "video": "MELD/videos/test_dia34_utt18.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh what do you know? Virgin! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "96",
        "video": "MELD/videos/test_dia147_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Come on, wussies! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "97",
        "video": "MELD/videos/test_dia175_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Those contracts absolutely had to go out today! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "98",
        "video": "MELD/videos/test_dia106_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh yeah, Smokey Joe here got half way to the highway and collapsed. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "99",
        "video": "MELD/videos/test_dia274_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, are you just going down there to gawk at that hot girl with the belly button ring again? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "100",
        "video": "MELD/videos/test_dia200_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: We’re trying to think! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "101",
        "video": "MELD/videos/test_dia259_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Is not. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "102",
        "video": "MELD/videos/test_dia113_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I'm not a dropper! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "103",
        "video": "MELD/videos/test_dia160_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Is that too much to ask after six years?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "104",
        "video": "MELD/videos/test_dia175_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Y’know what Tag, if we went down to the office you would see those contracts sitting on your desk. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "105",
        "video": "MELD/videos/test_dia90_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: She is not…  She’s gay. Oh my God. She is You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "106",
        "video": "MELD/videos/test_dia109_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Because you know, you could make little puppets out of them, and you could use them in your theater of cruelty. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "107",
        "video": "MELD/videos/test_dia17_utt30.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, well you are just gonna have too, okay. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "108",
        "video": "MELD/videos/test_dia279_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, fade the accent out and people will think you’re, y’know, that you’re adjusting to life in America. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "109",
        "video": "MELD/videos/test_dia147_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You guys are always hanging out in my apartment! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "110",
        "video": "MELD/videos/test_dia166_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No! She had the ring on, she seemed so excited, and then she  took her breast out. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "111",
        "video": "MELD/videos/test_dia184_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What are you looking at?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "112",
        "video": "MELD/videos/test_dia73_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, maybe that’s because you’re closer to you. So you look bigger to you from where you are. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "113",
        "video": "MELD/videos/test_dia9_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Joey! He’s working!  You would look You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "114",
        "video": "MELD/videos/test_dia204_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Look at this, look. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "115",
        "video": "MELD/videos/test_dia252_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Does that seem like something you can do. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "116",
        "video": "MELD/videos/test_dia3_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, fine, whatever. Welcome to the building. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "117",
        "video": "MELD/videos/test_dia12_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What’s all this about you guys fighting?! Is this really over a room?! I mean, that is so silly! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "118",
        "video": "MELD/videos/test_dia95_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, but they didn't ask me! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "119",
        "video": "MELD/videos/test_dia210_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Chandler, we said we would meet at the coffeehouse at six. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "120",
        "video": "MELD/videos/test_dia70_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You know, this isn't fair. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "121",
        "video": "MELD/videos/test_dia41_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It’s Joshua. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "122",
        "video": "MELD/videos/test_dia204_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: She buys tickets for plays that I wanna see. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "123",
        "video": "MELD/videos/test_dia254_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: All right, I'm going. I'm going. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "124",
        "video": "MELD/videos/test_dia56_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No, I don’t see it! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "125",
        "video": "MELD/videos/test_dia211_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That’s my wife!!! Get out! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "126",
        "video": "MELD/videos/test_dia244_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Sure, your dresser is missing but this she notices. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "127",
        "video": "MELD/videos/test_dia240_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What are you going to do? Kill him? Like you did with Charles?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "128",
        "video": "MELD/videos/test_dia102_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh, well, this is just perfect! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "129",
        "video": "MELD/videos/test_dia159_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The only person that should feel awkward is you, and you didn’t tell him not to call me, did you? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "130",
        "video": "MELD/videos/test_dia2_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, y'know what? There is no more left, left! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "131",
        "video": "MELD/videos/test_dia69_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Excuse me, look, we've been here for over an hour, and a lot of people less sick than my friend have gone in. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "132",
        "video": "MELD/videos/test_dia168_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: We know you took so just-just save yourself the time and confess! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "133",
        "video": "MELD/videos/test_dia160_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I mean, all I’m asking for is just a little emotion! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "134",
        "video": "MELD/videos/test_dia204_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, well I might be if I had the supplies! I mean, I could do all this stuff. Only I don't. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "135",
        "video": "MELD/videos/test_dia242_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It's been fourteen and a half minutes and you still have not said one word. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "136",
        "video": "MELD/videos/test_dia196_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What the hell! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "137",
        "video": "MELD/videos/test_dia2_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh okay, lift it straight up over your head! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "138",
        "video": "MELD/videos/test_dia134_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Stupid, useless Canadian money! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "139",
        "video": "MELD/videos/test_dia252_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: But this break-up was not all my fault, and she, she says here,  “If you accept full responsibility...”  Full responsibility! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "140",
        "video": "MELD/videos/test_dia168_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Look, I don’t need to steal some stupid ring, all right? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "141",
        "video": "MELD/videos/test_dia10_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: There’s just so much pressure. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "anger"
            }
        ]
    },
    {
        "id": "142",
        "video": "MELD/videos/test_dia130_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey, did I do these neat enough? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "143",
        "video": "MELD/videos/test_dia111_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ah, the forbidden love of a man and his door. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "144",
        "video": "MELD/videos/test_dia47_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "145",
        "video": "MELD/videos/test_dia196_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Magic 8 Ball, should I never see Rachel again? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "146",
        "video": "MELD/videos/test_dia203_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Because I think about shaving it all off again sometime. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "147",
        "video": "MELD/videos/test_dia241_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, I understand. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "148",
        "video": "MELD/videos/test_dia71_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: So there you go, that’s me. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "149",
        "video": "MELD/videos/test_dia226_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Although y’know, he-he’s a private guy. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "150",
        "video": "MELD/videos/test_dia244_utt19.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, better, actually. Y'know I-I-I think I finally figured out why we were having so much trouble lately. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "151",
        "video": "MELD/videos/test_dia73_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: All right. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "152",
        "video": "MELD/videos/test_dia85_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: But um, I don’t think it’s anything serious. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "153",
        "video": "MELD/videos/test_dia197_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Are you Phoebe Buffay? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "154",
        "video": "MELD/videos/test_dia43_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: All right Pheebs, stick out your plate! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "155",
        "video": "MELD/videos/test_dia100_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Me taking care of you is no problem, huh? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "156",
        "video": "MELD/videos/test_dia163_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I think she was. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "157",
        "video": "MELD/videos/test_dia28_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It’s kind of embarrassing. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "158",
        "video": "MELD/videos/test_dia232_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: So Chandler, have you heard about Monica's secret boyfriend? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "159",
        "video": "MELD/videos/test_dia137_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What are you doing tonight? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "160",
        "video": "MELD/videos/test_dia263_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Where've you been? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "161",
        "video": "MELD/videos/test_dia22_utt20.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I'm afraid so. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "162",
        "video": "MELD/videos/test_dia229_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: But if you’re thinking you can put a fish in there and it wouldn’t get sucked up into the mechanism, well you’d be wrong. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "163",
        "video": "MELD/videos/test_dia117_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Umm, hey, a couple of questions though. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "164",
        "video": "MELD/videos/test_dia98_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Can I borrow the phone? I want to call my apartment and check on my grandma.  What's my number? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "165",
        "video": "MELD/videos/test_dia109_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That's OK. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "166",
        "video": "MELD/videos/test_dia88_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Keep in mind, he's never used this product before, you're gonna see how easy this is to do. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "167",
        "video": "MELD/videos/test_dia49_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I'm You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "168",
        "video": "MELD/videos/test_dia39_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, he better not come by here. He can’t see the bride in the wedding dress. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "169",
        "video": "MELD/videos/test_dia240_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: An-an-anyway I-I just wanted to say that since I’m getting your brain when you leave the show, I was wondering if there was any tips you can give me You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "170",
        "video": "MELD/videos/test_dia243_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It’s my sister. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "171",
        "video": "MELD/videos/test_dia76_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No, I mean tonight, in the building. Suddenly. Into our lives. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "172",
        "video": "MELD/videos/test_dia137_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh!  Sorry. Uh, I’ve got those plans with Phoebe, why? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "173",
        "video": "MELD/videos/test_dia211_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, we should go. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "174",
        "video": "MELD/videos/test_dia247_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, right away Mr. Kaplan. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "175",
        "video": "MELD/videos/test_dia130_utt17.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Huh? Yeah, fine, invite whoever you want. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "176",
        "video": "MELD/videos/test_dia45_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Fair enough, now go get ready! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "177",
        "video": "MELD/videos/test_dia100_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I just have to clean that up. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "178",
        "video": "MELD/videos/test_dia234_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, it’s coffee. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "179",
        "video": "MELD/videos/test_dia17_utt23.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: So, I’ll invite him to brunch tomorrow and you can make nice. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "180",
        "video": "MELD/videos/test_dia12_utt17.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: And Mon, you wanna live with Chandler, don’t ya? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "181",
        "video": "MELD/videos/test_dia132_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hands. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "182",
        "video": "MELD/videos/test_dia242_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Would you like to call somebody? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "183",
        "video": "MELD/videos/test_dia226_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No that’s what you do when you want to get the truth out of someone. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "184",
        "video": "MELD/videos/test_dia168_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: All right, okay, this is great, uh, Chandler, you get behind the desk. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "185",
        "video": "MELD/videos/test_dia144_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: So it’s really a question of You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "186",
        "video": "MELD/videos/test_dia189_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What do you think of the Abbey, Chandler? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "187",
        "video": "MELD/videos/test_dia15_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Bye! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "188",
        "video": "MELD/videos/test_dia231_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I love you both. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "189",
        "video": "MELD/videos/test_dia266_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I don't to go through with this if it's going to raise the question of \"Us.\" You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "190",
        "video": "MELD/videos/test_dia49_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Monica? What time is it? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "191",
        "video": "MELD/videos/test_dia163_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: And what you said about Nana. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "192",
        "video": "MELD/videos/test_dia177_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: To see if know what these guys are talking about. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "193",
        "video": "MELD/videos/test_dia58_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Y’know, I don’t have an appointment, but I sure could use a physical.  Are you sure you’re okay? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "194",
        "video": "MELD/videos/test_dia244_utt18.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Umm, how’s it going with you guys? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "195",
        "video": "MELD/videos/test_dia44_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Diane Keaton. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "196",
        "video": "MELD/videos/test_dia124_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: This is the final boarding call for Flight 664 to Yemen. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "197",
        "video": "MELD/videos/test_dia20_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: See look, You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "198",
        "video": "MELD/videos/test_dia7_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "199",
        "video": "MELD/videos/test_dia93_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Joey, Joey, I-I-I’ll g-get you your job back if you help me out. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "200",
        "video": "MELD/videos/test_dia70_utt18.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hi, is Molly here? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "201",
        "video": "MELD/videos/test_dia107_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Honey, I was wondering.... You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "202",
        "video": "MELD/videos/test_dia33_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Umm, she Rach, not it, she. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "203",
        "video": "MELD/videos/test_dia116_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: She's not much of a phone person. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "204",
        "video": "MELD/videos/test_dia270_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What's it look like? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "205",
        "video": "MELD/videos/test_dia193_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, why don’t we all uh,  open our books to page 23. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "206",
        "video": "MELD/videos/test_dia238_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I thought it was gonna be a closed casket. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "207",
        "video": "MELD/videos/test_dia267_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ah Joey. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "208",
        "video": "MELD/videos/test_dia34_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, you’re not. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "209",
        "video": "MELD/videos/test_dia186_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I understand: who would cancel an actual date to go to a fake bachelor party? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "210",
        "video": "MELD/videos/test_dia266_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: And um, what-what is that Ross? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "211",
        "video": "MELD/videos/test_dia88_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: And there is Kevin. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "212",
        "video": "MELD/videos/test_dia113_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: All right, let me in. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "neutral"
            }
        ]
    },
    {
        "id": "213",
        "video": "MELD/videos/test_dia68_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "214",
        "video": "MELD/videos/test_dia233_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, it's so much more fun with you. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "215",
        "video": "MELD/videos/test_dia58_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah! Totally! Totally, and you? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "216",
        "video": "MELD/videos/test_dia242_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: There you go! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "217",
        "video": "MELD/videos/test_dia189_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: All right! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "218",
        "video": "MELD/videos/test_dia53_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Unless, we give her all gifts she can use after she’s done being pregnant. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "219",
        "video": "MELD/videos/test_dia259_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Remember, evolution? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "220",
        "video": "MELD/videos/test_dia14_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey-hey-hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "221",
        "video": "MELD/videos/test_dia134_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh, man! Chicago, is sooo lucky! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "222",
        "video": "MELD/videos/test_dia48_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, cheerie-o! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "223",
        "video": "MELD/videos/test_dia70_utt22.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It's like my favorite fairy tale come true!  The princess, the stable boy and the lesbian! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "224",
        "video": "MELD/videos/test_dia156_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well now-now you’re just talking crazy. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "225",
        "video": "MELD/videos/test_dia56_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh. Oh, it’s beautiful. I see it now. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "226",
        "video": "MELD/videos/test_dia51_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hi! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "227",
        "video": "MELD/videos/test_dia171_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ross, I think we should stick to the routine, we don’t wanna look stupid! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "228",
        "video": "MELD/videos/test_dia73_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Whoa!  I kinda liked it. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "229",
        "video": "MELD/videos/test_dia245_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, just some pictures I made and hung up. I thought they’d brighten up the place. They do don’t you think? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "230",
        "video": "MELD/videos/test_dia210_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I know what time you said. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "231",
        "video": "MELD/videos/test_dia56_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, that’s our baby. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "232",
        "video": "MELD/videos/test_dia97_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Everybody smile!  Okay, thanks a lot! Enjoy your stay at You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "233",
        "video": "MELD/videos/test_dia215_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That’s good. I got to admit, I thought it was something else. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "234",
        "video": "MELD/videos/test_dia262_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Wow! Skates! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "235",
        "video": "MELD/videos/test_dia58_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "236",
        "video": "MELD/videos/test_dia171_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, here’s where we go to the live shot of Times Square, nice work everyone that’s a wrap! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "237",
        "video": "MELD/videos/test_dia133_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Awww. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "238",
        "video": "MELD/videos/test_dia240_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No. No, I’m Joey Tribbiani; we did a scene together yesterday. I-I’m the guy in the coma! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "239",
        "video": "MELD/videos/test_dia214_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "240",
        "video": "MELD/videos/test_dia193_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I’m on time! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "241",
        "video": "MELD/videos/test_dia125_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hi! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "242",
        "video": "MELD/videos/test_dia78_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Rach, look!  Oh, hi! Where is my strong Ross Skywalker to come rescue me.  There he is. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "243",
        "video": "MELD/videos/test_dia121_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I love you too. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "244",
        "video": "MELD/videos/test_dia28_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Let’s just say there was a typographical error with a sex manual. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "245",
        "video": "MELD/videos/test_dia226_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Good. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "246",
        "video": "MELD/videos/test_dia255_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hello? Hello? Oh! Rachel, it's Paolo calling from Rome. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "247",
        "video": "MELD/videos/test_dia136_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God! That is so good! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "248",
        "video": "MELD/videos/test_dia112_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I hope you don’t think I’m crazy but I feel like I was meant to pick this up, do you believe in that kind of thing? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "249",
        "video": "MELD/videos/test_dia193_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yes, I made it! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "250",
        "video": "MELD/videos/test_dia266_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The physical act of love. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "251",
        "video": "MELD/videos/test_dia254_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I can't belive it, I'm gonna be an aunt. I'm gonna have like a nephew. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "252",
        "video": "MELD/videos/test_dia222_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I’ve been waiting for this for months! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "253",
        "video": "MELD/videos/test_dia104_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Excellent! Excellent! And uh, finally, are you into any weird stuff y’know, sexually? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "254",
        "video": "MELD/videos/test_dia22_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You deal with it and move on! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "255",
        "video": "MELD/videos/test_dia154_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey!  Hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "256",
        "video": "MELD/videos/test_dia60_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: oh, oh, yeah, ok, thanks. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "257",
        "video": "MELD/videos/test_dia233_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: We did have fun, didn't we? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "258",
        "video": "MELD/videos/test_dia259_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I-I studied evolution. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "259",
        "video": "MELD/videos/test_dia29_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No, but I want...I want the pinecones! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "260",
        "video": "MELD/videos/test_dia120_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: With a mahogany sleigh bed and bedside tables with flowers on them all the time! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "261",
        "video": "MELD/videos/test_dia133_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, you’re welcome a million. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "262",
        "video": "MELD/videos/test_dia56_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Come on! Come on! Here, okay-okay, you see this?  This tiny thing that looks like a peanut? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "263",
        "video": "MELD/videos/test_dia261_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "264",
        "video": "MELD/videos/test_dia188_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey, that woman's got an ass like Carol's!  What? Thought we were trying to find stuff. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "265",
        "video": "MELD/videos/test_dia214_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I mean, y’know, I’m not really that comfortable dancing with a— We-he!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "266",
        "video": "MELD/videos/test_dia247_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I know! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "267",
        "video": "MELD/videos/test_dia167_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Excellent! What Rachel has shrewdly observed here You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "268",
        "video": "MELD/videos/test_dia229_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, life’s pretty great isn’t it? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "269",
        "video": "MELD/videos/test_dia154_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hi! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "270",
        "video": "MELD/videos/test_dia161_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That’s so great for you guys! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "271",
        "video": "MELD/videos/test_dia222_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I got new sheets! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "272",
        "video": "MELD/videos/test_dia15_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah! Sure! Yep!  Oh, y'know what? If I heard a shot right now, I'd throw my body on you. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "273",
        "video": "MELD/videos/test_dia57_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Will you stop! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "274",
        "video": "MELD/videos/test_dia96_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Whaddya think? Today's the big day! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "275",
        "video": "MELD/videos/test_dia255_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Monica, your dad just beeped in, but can you make it quick? Talking to Rome.  I'm talking to Rome. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "276",
        "video": "MELD/videos/test_dia49_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Y'know we'd have no interruptions and we could be naked the entire time. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "277",
        "video": "MELD/videos/test_dia232_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Because I'm very happy for him!  And You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "278",
        "video": "MELD/videos/test_dia2_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You can do it! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "279",
        "video": "MELD/videos/test_dia154_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay. Okay, here’s a good one of us. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "280",
        "video": "MELD/videos/test_dia101_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "281",
        "video": "MELD/videos/test_dia15_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh yeah? Well maybe you and I should take a walk through a bad neighborhood. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "282",
        "video": "MELD/videos/test_dia96_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Thank you! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "283",
        "video": "MELD/videos/test_dia202_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh, listen. I’ve got to get going. Today was great, thanks! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "joy"
            }
        ]
    },
    {
        "id": "284",
        "video": "MELD/videos/test_dia47_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah but we won’t be able to like get up in the middle of the night and have those long talks about our feelings and the future. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "285",
        "video": "MELD/videos/test_dia225_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Of course not. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "286",
        "video": "MELD/videos/test_dia58_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I mean, I remember a time when You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "287",
        "video": "MELD/videos/test_dia199_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: And I-I am just trying to figure out why. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "288",
        "video": "MELD/videos/test_dia199_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Again. Y’know what? I think we all did. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "289",
        "video": "MELD/videos/test_dia167_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, honey that was pretty obvious. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "290",
        "video": "MELD/videos/test_dia195_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: So I moved everything around and then I forgot where it, where it went back and I'm sorry, I'm very sorry, I'm sorry, I'm sorry. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "291",
        "video": "MELD/videos/test_dia58_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh yeah! Yeah! Don’t worry about me, I’ll be fine! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "292",
        "video": "MELD/videos/test_dia93_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I can’t believe I’m going to lose this job! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "293",
        "video": "MELD/videos/test_dia133_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, I-I gotta go break up with Bonnie. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "294",
        "video": "MELD/videos/test_dia56_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I know, I lied! I didn’t want her to think I was a terrible mother! I can’t even see my own baby! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "295",
        "video": "MELD/videos/test_dia100_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I didn’t mean to scare you. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "296",
        "video": "MELD/videos/test_dia123_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay. But the question is who’s gonna go first. ‘Cause whoever goes second is the bitch. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "297",
        "video": "MELD/videos/test_dia208_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I’ve just been thinking about how my baby and I are gonna be all alone. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "298",
        "video": "MELD/videos/test_dia147_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Uh, it's 2:30 in the morning! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "299",
        "video": "MELD/videos/test_dia41_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, y’know what, you were right, you were right. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "300",
        "video": "MELD/videos/test_dia47_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No more J and C’s. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "301",
        "video": "MELD/videos/test_dia155_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God. I cannot keep having this same fight with you Ross! Look, urrgh, maybe we should take a break. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "302",
        "video": "MELD/videos/test_dia208_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What is it? Hey! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "303",
        "video": "MELD/videos/test_dia53_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Because she’s not gonna get to keep the babies. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "304",
        "video": "MELD/videos/test_dia100_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I broke it. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "305",
        "video": "MELD/videos/test_dia107_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "306",
        "video": "MELD/videos/test_dia232_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hi! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "307",
        "video": "MELD/videos/test_dia33_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh  oh no just stopped to throw up a little bit. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "308",
        "video": "MELD/videos/test_dia165_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I can’t believe we’re living here! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "309",
        "video": "MELD/videos/test_dia119_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey, you guys. Listen, I’m sorry that I was hogging the game before——Oh my God! Your friends have some unfortunate initials! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "310",
        "video": "MELD/videos/test_dia158_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why?! Why?! He said he’d call. Why hasn’t he called? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "311",
        "video": "MELD/videos/test_dia22_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Because of the weekend, we had a fight. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "312",
        "video": "MELD/videos/test_dia32_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, poor Lily. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "313",
        "video": "MELD/videos/test_dia214_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: : I’m sure as hell a dancer, it’s no use Marge will never go for me. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "314",
        "video": "MELD/videos/test_dia130_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I don’t think I’d be comfortable with any of my old lovers there. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "315",
        "video": "MELD/videos/test_dia2_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Uhh, yes I did but there isn't. Okay, here we go. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "316",
        "video": "MELD/videos/test_dia9_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, it’s bad. It’s really bad. The only thing in there that isn’t burned is an ass. Which I do You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "317",
        "video": "MELD/videos/test_dia41_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: We really weren’t great at being guys, but you know why? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "318",
        "video": "MELD/videos/test_dia215_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I-I do though. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "319",
        "video": "MELD/videos/test_dia116_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Good luck, and uh, take care, huh? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "320",
        "video": "MELD/videos/test_dia215_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You’re right! You’re right. Don’t say it. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "321",
        "video": "MELD/videos/test_dia118_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, it just hurts so bad. I uh You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "322",
        "video": "MELD/videos/test_dia53_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God! We are throwing the most depressing baby shower ever! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "323",
        "video": "MELD/videos/test_dia238_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, that doesn't mean she can't look nice! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "324",
        "video": "MELD/videos/test_dia33_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh come on, what am I gonna do, its been hours and it  won’t stop crying. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "325",
        "video": "MELD/videos/test_dia150_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Honey, honey, people are looking. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "326",
        "video": "MELD/videos/test_dia118_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh, I’m sorry. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "327",
        "video": "MELD/videos/test_dia263_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: She says as time goes on, he's gonna start getting agressive and violent. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "328",
        "video": "MELD/videos/test_dia174_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really? I’ve been thinking, this is crazy, I mean don’t, don’t you think we can work on this? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "329",
        "video": "MELD/videos/test_dia129_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, people! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "330",
        "video": "MELD/videos/test_dia130_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, no-no, y’know I absolutely adore Rachel it’s just that, well it might be a awkward for you. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "331",
        "video": "MELD/videos/test_dia47_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: God, it’s gonna so weird like when I come home and you’re not here. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "332",
        "video": "MELD/videos/test_dia93_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh I’m so sorry man! Is there anything I can do? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "333",
        "video": "MELD/videos/test_dia68_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I can't believe you would say that! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "334",
        "video": "MELD/videos/test_dia199_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, how did it go? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "335",
        "video": "MELD/videos/test_dia240_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Can I get some ice here?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "336",
        "video": "MELD/videos/test_dia78_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "337",
        "video": "MELD/videos/test_dia150_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Good-bye Chandler Bing. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "338",
        "video": "MELD/videos/test_dia156_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Good God Bing I…well I can’t say I’m altogether surprised, I saw the way she looked at you, and there was no love there. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "339",
        "video": "MELD/videos/test_dia199_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I guess I just figured that somewhere down the road, we would be on again. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "340",
        "video": "MELD/videos/test_dia199_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, maybe, but I don’t think I even care. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "341",
        "video": "MELD/videos/test_dia118_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, I wish there was something I can do to make you feel better. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "342",
        "video": "MELD/videos/test_dia208_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I just never thought I would raise this baby all by myself. Pretty dumb huh? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "343",
        "video": "MELD/videos/test_dia90_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I’m sorry your wife is gay. I guess women aren’t that great either. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "344",
        "video": "MELD/videos/test_dia163_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh, yeah she really would’ve wanted to be there. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "345",
        "video": "MELD/videos/test_dia214_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Come on Treeger, don’t say that. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "346",
        "video": "MELD/videos/test_dia9_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Everything’s ruined. My bed. My clothes. Look at my favorite blue sweater. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "347",
        "video": "MELD/videos/test_dia173_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I will! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "348",
        "video": "MELD/videos/test_dia184_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, and it really freaked me out! And after a while I even tried to hurt you and it just spurred you on. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "349",
        "video": "MELD/videos/test_dia32_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh, y'know I-I heard about what happened, that must have been just terrible for you, losing your mother that way. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "350",
        "video": "MELD/videos/test_dia111_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really stupid. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "351",
        "video": "MELD/videos/test_dia160_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It’s like the end of an era! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "352",
        "video": "MELD/videos/test_dia207_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ah, oh God. Oh, honey, oh that's OK. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "353",
        "video": "MELD/videos/test_dia22_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, y'know, you and me, it had to end sometime. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "354",
        "video": "MELD/videos/test_dia28_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Car accident. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "sadness"
            }
        ]
    },
    {
        "id": "355",
        "video": "MELD/videos/test_dia17_utt17.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Monica kinda trusted me with something and she shouldn’t have! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "356",
        "video": "MELD/videos/test_dia126_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No-no! Big bear! Big bear outside! I think I-I—would you—actually, would you go check on that? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "357",
        "video": "MELD/videos/test_dia178_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: When you’re done over there, we kinda have a situation over here too. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "358",
        "video": "MELD/videos/test_dia99_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Noo!  Oh, it’s okay, it’s okay, baby, baby, baby. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "359",
        "video": "MELD/videos/test_dia182_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Help! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "360",
        "video": "MELD/videos/test_dia50_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I’m so sorry! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "361",
        "video": "MELD/videos/test_dia231_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, good, good, good, ‘cause, good, ‘cause I was kinda having second thoughts too. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "362",
        "video": "MELD/videos/test_dia103_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Do you have a minute? I’d like to talk to you about something I’m, I’m really uncomfortable talking about. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "363",
        "video": "MELD/videos/test_dia153_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It have a fear of heights, you go to the top of the building! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "364",
        "video": "MELD/videos/test_dia119_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: But Ben is coming over tonight and he can’t see this. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "365",
        "video": "MELD/videos/test_dia73_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ross-Ross-Ross-Ross! Stay away from that guy , and that guy . And that one—Dude! They’re all huge! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "366",
        "video": "MELD/videos/test_dia223_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I’m not worried, I’m uh, I’m fascinated. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "367",
        "video": "MELD/videos/test_dia182_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The charity's on fire! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "368",
        "video": "MELD/videos/test_dia245_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No-no-no, no! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "369",
        "video": "MELD/videos/test_dia111_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Are we okay now? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "370",
        "video": "MELD/videos/test_dia3_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No wait, look. Look! I'm sorry, it's just I've never even You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "371",
        "video": "MELD/videos/test_dia263_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: She's not gonna make you wear one of those big plastic cones, is she? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "372",
        "video": "MELD/videos/test_dia128_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Brenda a bee! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "373",
        "video": "MELD/videos/test_dia113_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No-no! Don't do it! Don't! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "374",
        "video": "MELD/videos/test_dia100_utt18.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Where are you Leslie? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "375",
        "video": "MELD/videos/test_dia130_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Wait-wait-wait, do you, do you think, maybe we shouldn’t invite her? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "376",
        "video": "MELD/videos/test_dia271_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I'm trppd... in an ATM vstbl... wth You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "377",
        "video": "MELD/videos/test_dia153_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, Joe, I assure you if I'm staring down the barrel of a gun, I'm pretty much peeing every which way. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "378",
        "video": "MELD/videos/test_dia118_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: This actor friend and he would kill me if he thought I was doing this umm, but umm would it be possible for him to get an audition for your movie say on Thursday? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "379",
        "video": "MELD/videos/test_dia143_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Are you gonna break up with me if I get fat again? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "380",
        "video": "MELD/videos/test_dia29_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ok, go quick! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "381",
        "video": "MELD/videos/test_dia168_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Where’s my ring? My dead grandmother’s wedding ring? Where is it? Where is it? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "382",
        "video": "MELD/videos/test_dia117_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Umm, about that-that-that last move where the woman tripped you and then pinned you to the floor, what-what-what-what would you do next? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "383",
        "video": "MELD/videos/test_dia175_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No seriously, y’know the contracts I gave you, did you overnight them? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "384",
        "video": "MELD/videos/test_dia71_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I tell ya, that has never happened before. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "385",
        "video": "MELD/videos/test_dia195_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay look, don't...don't be mad okay? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "386",
        "video": "MELD/videos/test_dia17_utt16.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I’m, I’m freaking out! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "387",
        "video": "MELD/videos/test_dia184_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my—This is so embarrassing. Oh my God, I’m never gonna get massaged again! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "388",
        "video": "MELD/videos/test_dia133_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I can’t-I can’t stay here all night, and if I go in there she’s-she’s gonna wanna... do stuff. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "389",
        "video": "MELD/videos/test_dia250_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well, I y'know, I was just, umm, I was just thinking and hoping, that umm, maybe you’d want to get back together? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "390",
        "video": "MELD/videos/test_dia4_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God! I overslept! I was supposed to be on the set a half an hour ago! I gotta get out of here! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "391",
        "video": "MELD/videos/test_dia97_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Uh-oh! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "392",
        "video": "MELD/videos/test_dia217_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: But we-we didn’t have…sex-uh, did we? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "393",
        "video": "MELD/videos/test_dia29_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Look, we have enough, just walk away. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "394",
        "video": "MELD/videos/test_dia105_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No! No!! It’s me! It’s me! I-I didn’t want to make any noise! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "395",
        "video": "MELD/videos/test_dia205_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God. You guys! You gotta come see this! There's some creep out there with a telescope! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "396",
        "video": "MELD/videos/test_dia95_utt17.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Umm, see, I was thinking maybe you two could switch apartments because Phoebe's more our kind of people. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "397",
        "video": "MELD/videos/test_dia11_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I mean there’s no way Joey’s gonna make it in time. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "398",
        "video": "MELD/videos/test_dia100_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why are there only two of you? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "399",
        "video": "MELD/videos/test_dia9_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Coming through!  Oh! Coming through!  Oh! Hello! Hi!  No! Right! Coming through! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "400",
        "video": "MELD/videos/test_dia100_utt15.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Where is Leslie? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "401",
        "video": "MELD/videos/test_dia227_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, Ben, I won’t tell your daddy that you had ice cream for dinner, if you don’t tell about our little bonking incident. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "402",
        "video": "MELD/videos/test_dia60_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, but what if it's not? What if there    is a reason why we can't have a baby? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "403",
        "video": "MELD/videos/test_dia37_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: And you got a 'hate' from that?! Your taking a big leap there... You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "404",
        "video": "MELD/videos/test_dia223_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Which is funny because in high school I uh, I-I failed Biology and tonight Biology failed me. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "fear"
            }
        ]
    },
    {
        "id": "405",
        "video": "MELD/videos/test_dia169_utt18.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: About a month ago this guy spent the night with Rachel, I didn’t see who it was but You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "406",
        "video": "MELD/videos/test_dia186_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ah, no-no-no just Ross. Ross and Joey is embarrassing enough. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "407",
        "video": "MELD/videos/test_dia165_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I don’t know! But maybe if we keep that drawer shut, it’ll die. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "408",
        "video": "MELD/videos/test_dia57_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: But you will always be the guy who peed on me. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "409",
        "video": "MELD/videos/test_dia17_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ewww! Oh! It’s the Mattress King! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "410",
        "video": "MELD/videos/test_dia205_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I feel violated. And not in a good way. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "411",
        "video": "MELD/videos/test_dia123_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Then he comes over!  \"I’m so worried about you.\" Uck! Be a man! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "412",
        "video": "MELD/videos/test_dia233_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Honey, if you wanted to do that, you might as well just gotten him a fish, you know how fish freaked me out! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "413",
        "video": "MELD/videos/test_dia120_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, I’m sorry. Of course I mean that. Interesting idea, umm, talk about it, but no. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "414",
        "video": "MELD/videos/test_dia128_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Look, I know it must be hard that your wife is a lesbian, but it’s wrong. You’re married. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "415",
        "video": "MELD/videos/test_dia123_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No! No-no! He is totally incompetent. I called the chef who recommended him to me. He said, \"Ha-ha! Gotcha!\" You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "416",
        "video": "MELD/videos/test_dia210_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my good God. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "417",
        "video": "MELD/videos/test_dia17_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Don’t look honey. Change the channel! Change the channel! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "418",
        "video": "MELD/videos/test_dia196_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: A Magic Eight ball?! You can't be serious, you can't make this decision with a toy! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "419",
        "video": "MELD/videos/test_dia4_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh wait, Joey, you can’t go like that! You stink! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "420",
        "video": "MELD/videos/test_dia70_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What do you think you're gonna do, have sex with her right here    on my couch? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "421",
        "video": "MELD/videos/test_dia106_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Then why are you smoking? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "422",
        "video": "MELD/videos/test_dia244_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The spitter. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "423",
        "video": "MELD/videos/test_dia112_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I know, but that creep that I went on that date with goes to there so I have to find a new one. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "424",
        "video": "MELD/videos/test_dia34_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, it’s so easy for you I mean, you’re not married, you get to have sex with who ever you want! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "425",
        "video": "MELD/videos/test_dia115_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Shielding your face and shrieking like a girl... is not a backhand. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "426",
        "video": "MELD/videos/test_dia205_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, that is so sick. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "427",
        "video": "MELD/videos/test_dia108_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ugh! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "428",
        "video": "MELD/videos/test_dia278_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Someone on the subway licked my neck! Licked my neck!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "429",
        "video": "MELD/videos/test_dia96_utt7.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: C'mon, we're roommates!  My eyes!! My eyes!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "430",
        "video": "MELD/videos/test_dia259_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: The joke is not funny, and it’s offensive to women, and doctors, and monkeys! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "431",
        "video": "MELD/videos/test_dia17_utt12.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Four percent. Okay. I tip more than that when there’s a bug in my food. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "432",
        "video": "MELD/videos/test_dia128_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I think I know what’s going on here. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "433",
        "video": "MELD/videos/test_dia230_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ewww! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "434",
        "video": "MELD/videos/test_dia170_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I wanna see what’s in your hand. I wanna see the trash. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "435",
        "video": "MELD/videos/test_dia70_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No ... the leather sticks to my ass. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "436",
        "video": "MELD/videos/test_dia80_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well he made Rachel cry! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "437",
        "video": "MELD/videos/test_dia123_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Are you kidding?! Another week with that sip, I’ll kill You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "438",
        "video": "MELD/videos/test_dia244_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Well let’s see, there was the guy with the ferrets, that’s plural. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "439",
        "video": "MELD/videos/test_dia61_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Look, not that I enjoy talking about people who I went to high school with, ‘cause I do, but umm, maybe we could talk about something else? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "440",
        "video": "MELD/videos/test_dia231_utt10.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay. And it’s not just chicks y'know? It’s all kinds of other animals! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "441",
        "video": "MELD/videos/test_dia65_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No way are you cool enough to pull of Clint. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "442",
        "video": "MELD/videos/test_dia163_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, come on. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "443",
        "video": "MELD/videos/test_dia3_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ugh, can you believe that guy! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "444",
        "video": "MELD/videos/test_dia254_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, save it for the cab, okay. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "445",
        "video": "MELD/videos/test_dia170_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Eww! Oh, it’s all dirty. You should throw this out. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "446",
        "video": "MELD/videos/test_dia86_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It tastes like feet! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "447",
        "video": "MELD/videos/test_dia175_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Y’know, like the thing when you put the phone in your pants? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "448",
        "video": "MELD/videos/test_dia92_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Arghh!! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "449",
        "video": "MELD/videos/test_dia88_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You guys, can we please not watch this all right. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "450",
        "video": "MELD/videos/test_dia41_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No, I don’t. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "451",
        "video": "MELD/videos/test_dia180_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I mean,... how needy is that? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "452",
        "video": "MELD/videos/test_dia120_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: No. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "453",
        "video": "MELD/videos/test_dia231_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That’s horrible! Well, you did the right thing man. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "454",
        "video": "MELD/videos/test_dia237_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I don't know. We're talking about whipped fish, Monica. I'm just happy I'm keeping it down, y'know? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "455",
        "video": "MELD/videos/test_dia151_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, ew! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "456",
        "video": "MELD/videos/test_dia110_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Hey, y'know what you should do? You should take her back to the 1890's, when that phrase was last used. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "457",
        "video": "MELD/videos/test_dia52_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: To tape the game? You do this every time Ross, you’re You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "458",
        "video": "MELD/videos/test_dia17_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Yeah, I’m gonna go to a doctor who went to school in a mini-mall. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "459",
        "video": "MELD/videos/test_dia85_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why did I have to start working out again? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "460",
        "video": "MELD/videos/test_dia254_utt11.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, hating this. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "461",
        "video": "MELD/videos/test_dia259_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You are both idiots. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "462",
        "video": "MELD/videos/test_dia96_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God. Okay, go into the bathroom, use whatever you want, just don't ever tell me what you did in there. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "463",
        "video": "MELD/videos/test_dia34_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Say it louder, I don’t think the guy all the way in the back heard you! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "464",
        "video": "MELD/videos/test_dia30_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: You know what, where he hugs you and kinda rolls you away and... Oh... my....God. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "465",
        "video": "MELD/videos/test_dia220_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What’s that smell? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "466",
        "video": "MELD/videos/test_dia143_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: A hundred and forty-five pounds. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "467",
        "video": "MELD/videos/test_dia112_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: I also have to find a new video store, a new bank, a new adult bookstore, a new grocery store You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "468",
        "video": "MELD/videos/test_dia130_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh, God, nobody likes him, and he’s so cheap, he’d never fly to London in a million years. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "469",
        "video": "MELD/videos/test_dia138_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Who cares? I repel women. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "470",
        "video": "MELD/videos/test_dia237_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Eight and a half hours of aptitude tests, intelligence tests, personality tests... and what do I learn? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "471",
        "video": "MELD/videos/test_dia254_utt14.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: That's nice. Get out Let's go, come on. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "472",
        "video": "MELD/videos/test_dia175_utt18.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Or maybe you would see me looking embarrassed because you are talking on the phone with your crotch! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "disgust"
            }
        ]
    },
    {
        "id": "473",
        "video": "MELD/videos/test_dia0_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Why do all you’re coffee mugs have numbers on the bottom? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "474",
        "video": "MELD/videos/test_dia2_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Okay, go left. Left! Left! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "475",
        "video": "MELD/videos/test_dia4_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ohh! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "476",
        "video": "MELD/videos/test_dia4_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It kicked! I think the baby kicked! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "477",
        "video": "MELD/videos/test_dia6_utt2.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "478",
        "video": "MELD/videos/test_dia7_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What’s fish hooking? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "479",
        "video": "MELD/videos/test_dia8_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Bumpy? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "480",
        "video": "MELD/videos/test_dia9_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "481",
        "video": "MELD/videos/test_dia9_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: My God! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "482",
        "video": "MELD/videos/test_dia14_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Have you really done this before? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "483",
        "video": "MELD/videos/test_dia19_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Really? A purse? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "484",
        "video": "MELD/videos/test_dia21_utt6.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: So your brother's straigh huh? Seriously. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "485",
        "video": "MELD/videos/test_dia22_utt9.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Chandler that's crazy! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "486",
        "video": "MELD/videos/test_dia22_utt19.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: We're in a relationship? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "487",
        "video": "MELD/videos/test_dia25_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh God! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "488",
        "video": "MELD/videos/test_dia29_utt1.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, I didn't factor in the room tax. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "489",
        "video": "MELD/videos/test_dia29_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "490",
        "video": "MELD/videos/test_dia29_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: There's something new in the bowl. You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "491",
        "video": "MELD/videos/test_dia30_utt0.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God, what happened? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "492",
        "video": "MELD/videos/test_dia30_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Ross's what? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "493",
        "video": "MELD/videos/test_dia32_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh, what? What was it? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "494",
        "video": "MELD/videos/test_dia33_utt4.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: It worked! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "495",
        "video": "MELD/videos/test_dia34_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh my God! You’re a 30 year old virgin! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "496",
        "video": "MELD/videos/test_dia34_utt13.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Y’know what? You are right?! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "497",
        "video": "MELD/videos/test_dia41_utt3.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Who, Josh? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "498",
        "video": "MELD/videos/test_dia41_utt5.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: What, he doesn’t like Josh? You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    },
    {
        "id": "499",
        "video": "MELD/videos/test_dia41_utt8.mp4",
        "conversations": [
            {
                "from": "human",
                "value": "<video>\nThe person in video says: Oh no! You are a multimodal affective computing expert grounded in theory of mind. Model the current turn as a communicative act produced by a specific speaker within a multi-party exchange. Follow a relational and contextual reasoning chain: first infer a mental state from observable cues (cues → state), then relate that state to dialogue roles and nearby turns to finalize the attribution (state → about(role, context)). s(1) Decode Multimodal & Dialogic Cues (Perceptual Simulation): Systematically decode observable signals as outward evidence of an inner state—lexical content/transcript (if provided), speech prosody (if available), and visual behavior (face/gaze/posture/gesture if available). Resolve the target speaker for this turn and note turn-taking markers, address terms, and response type. (2) Attribute the Speaker’s Emotion (Cognitive Empathy, First-Order): Integrate the decoded cues to infer the speaker’s here-and-now emotional state from the speaker’s perspective (not the listener’s reaction and not a stable trait). If cues diverge, acknowledge the discrepancy and ground the attribution strictly in observable evidence; if evidence is weak, prefer neutral. (3) Contextualize with Roles & Surrounding Turns (Perspective-Taking & Relational Mapping): Use immediate context (preceding/following turns and participant roles such as addressee/third party) to refine or disambiguate the attribution—e.g., whether the turn is a reply, challenge, tease, agreement, or correction. Let context modulate, but not override, clear in-turn affective signals. Do not import external facts. (4) Conclude and Map to Label: Provide your reasoning following the steps above, then choose exactly one label from: neutral, surprise, fear, sadness, joy, disgust, anger. Output exactly: {'emotion': 'label'}."
            },
            {
                "from": "gpt",
                "value": "surprise"
            }
        ]
    }
]